{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cc5fc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"  # specify which GPU(s) to be used\n",
    "\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, HTML\n",
    "from llm_steer import LLM_Steer_Manager\n",
    "from measure_activation import Measure_Manager\n",
    "\n",
    "from utils import Config, Data_Manager, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9a532e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected LLM: Qwen/Qwen3-32B\n"
     ]
    }
   ],
   "source": [
    "model_ids = [\n",
    "    \"google/gemma-3-270m-it\", \n",
    "    \"Qwen/Qwen3-0.6B\", \n",
    "    \"meta-llama/Llama-3.2-1B-Instruct\", \n",
    "    \"Qwen/Qwen3-1.7B\", \n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "    \"google/gemma-3-4b-it\",\n",
    "    \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"google/gemma-3-12b-it\", \n",
    "    \"Qwen/Qwen3-14B\", \n",
    "    \"google/gemma-3-27b-it\",\n",
    "    \"Qwen/Qwen3-32B\",\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "]\n",
    "llm = model_ids[10]\n",
    "print(f'Selected LLM: {llm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685dab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveChat:\n",
    "    def __init__(self, cfg, model, tokenizer, dm, symp_label_dict, sae_dict, actmax_dict, device_dict, std_dict, generation_kwargs):\n",
    "        # ... (Your existing initialization code remains exactly the same) ...\n",
    "        self.cfg = cfg\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dm = dm\n",
    "        self.sae_dict = sae_dict\n",
    "        self.actmax_dict = actmax_dict\n",
    "        self.device_dict = device_dict\n",
    "        self.std_dict = std_dict\n",
    "        \n",
    "        self.symp_label_dict = symp_label_dict\n",
    "        self.symp_keys = list(symp_label_dict.keys())\n",
    "        \n",
    "        self.generation_kwargs = generation_kwargs\n",
    "        self.steer_manager = LLM_Steer_Manager(cfg, model, tokenizer, device_dict, self.generation_kwargs)\n",
    "        self.measure_manager = Measure_Manager(cfg, model, tokenizer, device_dict, std_dict, sae_dict, actmax_dict, self.symp_label_dict)\n",
    "        \n",
    "        self.chat_history = []\n",
    "        self.system_prompt = \"You are a helpful AI assistant.\"\n",
    "        if not self.chat_history:\n",
    "            self.chat_history.append({\"role\": \"system\", \"content\": self.system_prompt})\n",
    "\n",
    "        self.themes = {\n",
    "            'Default': {'user_color': '#0000ff', 'assistant_color': '#008000', 'bg_color': '#ffffff', 'text_color': '#000000'},\n",
    "            'Dark': {'user_color': '#4fc3f7', 'assistant_color': '#81c784', 'bg_color': '#1e1e1e', 'text_color': '#ffffff'},\n",
    "            'Blue': {'user_color': '#1565c0', 'assistant_color': '#00695c', 'bg_color': '#e3f2fd', 'text_color': '#0d47a1'}\n",
    "        }\n",
    "        self.current_theme = 'Default'\n",
    "\n",
    "        self.setup_ui()\n",
    "\n",
    "    def setup_ui(self):\n",
    "        # Theme Selector\n",
    "        self.theme_selector = widgets.Dropdown(\n",
    "            options=list(self.themes.keys()),\n",
    "            value='Default',\n",
    "            description='Theme:',\n",
    "            layout={'width': '200px'}\n",
    "        )\n",
    "        self.theme_selector.observe(self.change_theme, names='value')\n",
    "\n",
    "        # --- FIX: Chat Display Area ---\n",
    "        # Instead of Output, we use an HTML widget for content\n",
    "        self.chat_content = widgets.HTML(value=\"\", layout={'width': '100%'})\n",
    "        \n",
    "        # We wrap it in a VBox that handles the scrolling and borders\n",
    "        self.chat_container = widgets.VBox(\n",
    "            [self.chat_content], \n",
    "            layout={\n",
    "                'border': '1px solid #ccc', \n",
    "                'height': '400px', \n",
    "                'overflow_y': 'auto',  # Ensures scrollbar appears\n",
    "                'padding': '10px',\n",
    "                'margin': '0 0 20px 0',\n",
    "                'display': 'block'     # Helps VS Code render block correctly\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Input area\n",
    "        self.user_input = widgets.Textarea(\n",
    "            placeholder='Type your message here...',\n",
    "            layout={'width': '60%', 'height': '60px'}\n",
    "        )\n",
    "        \n",
    "        # Controls\n",
    "        self.send_button = widgets.Button(description=\"Send\", button_style='primary', icon='paper-plane', layout={'width': '100px'})\n",
    "        self.send_button.on_click(self.handle_submit)\n",
    "        \n",
    "        self.quit_button = widgets.Button(description=\"Quit\", button_style='danger', icon='times', layout={'width': '100px'})\n",
    "        self.quit_button.on_click(self.handle_quit)\n",
    "        \n",
    "        # Intervention Controls\n",
    "        self.use_intervention = widgets.Checkbox(value=False, description='Apply Intervention')\n",
    "        self.latent_selector = widgets.Dropdown(\n",
    "            options=self.symp_keys,\n",
    "            description='Latent:',\n",
    "            disabled=True,\n",
    "            layout={'width': '300px'}\n",
    "        )\n",
    "        \n",
    "        self.sliders = {}\n",
    "        slider_widgets = []\n",
    "        for layer in self.cfg.hook_layers:\n",
    "            slider = widgets.FloatSlider(\n",
    "                value=0.0, min=0.0, max=1.0, step=0.01,\n",
    "                description=f'Layer {layer}:', disabled=True,\n",
    "                continuous_update=False, orientation='horizontal',\n",
    "                readout=True, readout_format='.2f', layout={'width': '300px'}\n",
    "            )\n",
    "            self.sliders[layer] = slider\n",
    "            slider_widgets.append(slider)\n",
    "            \n",
    "        self.sliders_box = widgets.VBox(slider_widgets)\n",
    "        self.use_intervention.observe(self.toggle_intervention, names='value')\n",
    "        \n",
    "        # Layout\n",
    "        header = widgets.HBox([widgets.HTML(\"<h3>Interactive Chat with S3AE Intervention</h3>\"), self.theme_selector], layout={'justify_content': 'space-between', 'align_items': 'center', 'margin': '0 0 10px 0'})\n",
    "        \n",
    "        self.input_box = widgets.HBox([self.user_input, self.send_button, self.quit_button], layout={'align_items': 'center', 'margin': '10px 0'})\n",
    "        \n",
    "        self.intervention_controls = widgets.VBox([\n",
    "            widgets.HBox([self.use_intervention, self.latent_selector]),\n",
    "            widgets.Label(\"Intervention Strength per Layer:\"),\n",
    "            self.sliders_box\n",
    "        ], layout={'border': '1px solid #eee', 'padding': '10px', 'margin': '10px 0', 'background_color': '#f9f9f9'})\n",
    "        \n",
    "        self.main_layout = widgets.VBox([\n",
    "            header,\n",
    "            self.chat_container, # Use the new container\n",
    "            self.input_box,\n",
    "            self.intervention_controls\n",
    "        ])\n",
    "        \n",
    "    def get_max_pooled_measurements(self, text):\n",
    "            \"\"\"\n",
    "            Splits text into sentences, measures activations for each, \n",
    "            and returns the max activation per latent across all sentences.\n",
    "            \"\"\"\n",
    "            # 1. Split text into sentences using regex\n",
    "            # This regex splits by . ! ? but keeps the punctuation attached to the sentence\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "            \n",
    "            # Filter out empty strings/whitespace\n",
    "            sentences = [s.strip() for s in sentences if s.strip()]\n",
    "            \n",
    "            # Fallback: if no sentences detected (e.g. just a word), measure the whole text\n",
    "            if not sentences:\n",
    "                sentences = [text]\n",
    "\n",
    "            # 2. Get measurements for ALL sentences at once\n",
    "            # Assuming measure_manager returns a list of lists (one list of activations per input string)\n",
    "            sent_activations = self.measure_manager.sae_measure_no_json(sentences)\n",
    "\n",
    "            # 3. Max Pool across sentences\n",
    "            # Convert to numpy array for easy column-wise max operation\n",
    "            # Shape: (num_sentences, num_latents)\n",
    "            act_array = np.array(sent_activations)\n",
    "            \n",
    "            # Max along axis 0 (collapsing the sentences dimension)\n",
    "            # Shape: (num_latents,)\n",
    "            max_pooled = np.max(act_array, axis=0)\n",
    "            \n",
    "            return max_pooled.tolist()\n",
    "        \n",
    "    def toggle_intervention(self, change):\n",
    "        enabled = change['new']\n",
    "        self.latent_selector.disabled = not enabled\n",
    "        for slider in self.sliders.values():\n",
    "            slider.disabled = not enabled\n",
    "\n",
    "    def change_theme(self, change):\n",
    "        self.current_theme = change['new']\n",
    "        self.refresh_chat_display()\n",
    "\n",
    "    def handle_quit(self, b):\n",
    "        self.user_input.disabled = True\n",
    "        self.send_button.disabled = True\n",
    "        self.quit_button.disabled = True\n",
    "        self.use_intervention.disabled = True\n",
    "        self.theme_selector.disabled = True\n",
    "        # Append quit message to HTML\n",
    "        self.chat_content.value += \"<div style='margin-top:20px; text-align:center; color: #888;'><strong>--- Chat Ended ---</strong></div>\"\n",
    "\n",
    "    def handle_submit(self, b):\n",
    "        user_text = self.user_input.value.strip()\n",
    "        if not user_text:\n",
    "            return\n",
    "            \n",
    "        self.user_input.value = '' \n",
    "        \n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": user_text})\n",
    "        self.refresh_chat_display(typing=True)\n",
    "            \n",
    "        itv_settings = None\n",
    "        if self.use_intervention.value:\n",
    "            strengths = {layer: slider.value for layer, slider in self.sliders.items()}\n",
    "            itv_settings = {\n",
    "                'latent': self.latent_selector.value,\n",
    "                'strengths': strengths\n",
    "            }\n",
    "            \n",
    "        try:\n",
    "            if itv_settings:\n",
    "                response, measurements = self.generate_with_intervention(itv_settings)\n",
    "            else:\n",
    "                response, measurements = self.generate_normal()\n",
    "                \n",
    "            self.chat_history.append({\"role\": \"assistant\", \"content\": response, \"measurements\": measurements})\n",
    "            self.refresh_chat_display()\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Display error in the chat log in red\n",
    "            err_msg = f\"<div style='color: red; margin: 10px 0;'><strong>Error:</strong> {str(e)}</div>\"\n",
    "            current_html = self.chat_content.value\n",
    "            # Remove typing indicator if present\n",
    "            if \"Assistant is typing...\" in current_html:\n",
    "                 self.refresh_chat_display() # Redraw cleanly\n",
    "            self.chat_content.value += err_msg\n",
    "\n",
    "    def refresh_chat_display(self, typing=False):\n",
    "        theme = self.themes[self.current_theme]\n",
    "        bg_color = theme['bg_color']\n",
    "        text_color = theme['text_color']\n",
    "        user_color = theme['user_color']\n",
    "        asst_color = theme['assistant_color']\n",
    "        \n",
    "        # --- FIX: Removed clear_output logic ---\n",
    "        \n",
    "        # Build the full HTML string\n",
    "        html_content = f'<div style=\"background-color: {bg_color}; color: {text_color}; padding: 10px; min-height: 100%; font-family: sans-serif;\">'\n",
    "        \n",
    "        for msg in self.chat_history:\n",
    "            role = msg['role']\n",
    "            content = msg['content']\n",
    "            if role == 'system': continue\n",
    "            \n",
    "            # Formatting line breaks for HTML\n",
    "            content = content.replace('\\n', '<br>')\n",
    "            \n",
    "            if role == 'user':\n",
    "                html_content += f'<div style=\"margin: 10px 0;\"><strong><span style=\"color: {user_color}\">User:</span></strong> {content}</div>'\n",
    "            elif role == 'assistant':\n",
    "                html_content += f'<div style=\"margin: 10px 0;\"><strong><span style=\"color: {asst_color}\">Assistant:</span></strong> {content}</div>'\n",
    "                \n",
    "                if 'measurements' in msg and msg['measurements']:\n",
    "                    measurements = msg['measurements']\n",
    "                    measure_pairs = list(zip(self.symp_keys, measurements))\n",
    "                    measure_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "                    top_str = \", \".join([f\"{k}: {v:.2f}\" for k, v in measure_pairs[:5]])\n",
    "                    html_content += f'<div style=\"font-size: 0.9em; color: #888; margin-left: 20px;\"><em>Top Activations: {top_str}</em></div>'\n",
    "                    html_content += f'<hr style=\"border-color: #eee; margin: 5px 0;\">'\n",
    "        \n",
    "        if typing:\n",
    "            html_content += f'<div style=\"margin: 10px 0; color: #888;\"><em>Assistant is typing...</em></div>'\n",
    "        \n",
    "        html_content += '</div>'\n",
    "        \n",
    "        # Simply update the widget value\n",
    "        self.chat_content.value = html_content\n",
    "\n",
    "    # ... (Keep get_clean_history, generate_with_intervention, generate_normal, and start as they were) ...\n",
    "    def get_clean_history(self):\n",
    "        return [{'role': m['role'], 'content': m['content']} for m in self.chat_history]\n",
    "\n",
    "    def generate_with_intervention(self, settings):\n",
    "            latent = settings['latent']\n",
    "            strengths = settings['strengths']\n",
    "            \n",
    "            itv_W_dict = {}\n",
    "            itv_str_dict = {}\n",
    "            \n",
    "            latent_idx = self.symp_label_dict[latent]\n",
    "            \n",
    "            for layer in self.cfg.hook_layers:\n",
    "                # Get steering vector from SAE decoder\n",
    "                steering_vec = self.sae_dict[layer].decoder.weight.T[latent_idx]\n",
    "                steering_vec = steering_vec.to(self.device_dict[layer])\n",
    "                \n",
    "                # Reshape for LLM_Steer_Manager: (batch_size, 1, hidden_size)\n",
    "                itv_W_dict[layer] = steering_vec.unsqueeze(0).unsqueeze(0).to(torch.bfloat16)\n",
    "                \n",
    "                # Strength: (batch_size, 1)\n",
    "                s = strengths.get(layer, 0.0)\n",
    "                itv_str_dict[layer] = torch.tensor([[s]], device=self.device_dict[layer], dtype=torch.bfloat16)\n",
    "                \n",
    "            # Generate\n",
    "            output_tokens = self.steer_manager.generate_text_w_itv([self.get_clean_history()], itv_W_dict, itv_str_dict)\n",
    "            output_text = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "            \n",
    "            # --- CHANGED: Use max pooling helper ---\n",
    "            measurements = self.get_max_pooled_measurements(output_text)\n",
    "            \n",
    "            return output_text, measurements\n",
    "\n",
    "    def generate_normal(self):\n",
    "        output_tokens = self.steer_manager.generate_text([self.get_clean_history()])\n",
    "        output_text = self.tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "        \n",
    "        # --- CHANGED: Use max pooling helper ---\n",
    "        measurements = self.get_max_pooled_measurements(output_text)\n",
    "        \n",
    "        return output_text, measurements\n",
    "        \n",
    "    def start(self):\n",
    "        display(self.main_layout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ef859db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c2498de9164a92b00e2e1a87184271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfg = Config(llm)\n",
    "dm = Data_Manager(cfg)\n",
    "\n",
    "model, tokenizer = model_selection(cfg)\n",
    "sae_dict = dm.load_dict(dict_type='sae')\n",
    "actmax_dict = dm.load_dict(dict_type='actmax')\n",
    "device_dict = dm.load_dict(dict_type='device', model=model)\n",
    "std_dict = dm.load_dict(dict_type='act-std')\n",
    "for layer in cfg.hook_layers:\n",
    "    sae_dict[layer] = sae_dict[layer].to(device_dict[layer])\n",
    "    \n",
    "symp_label_dict, _, symp_keys, _ = dm.load_dict('label')\n",
    "\n",
    "generation_kwargs = {'max_new_tokens': 300, 'tmp': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872b7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_system = InteractiveChat(cfg, model, tokenizer, dm, symp_label_dict, sae_dict, actmax_dict, device_dict, std_dict, generation_kwargs)\n",
    "chat_system.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

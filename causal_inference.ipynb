{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import scipy.stats as statsc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pingouin import partial_corr\n",
    "\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_agraph import graphviz_layout, pygraphviz_layout\n",
    "from networkx.drawing.nx_pydot import pydot_layout\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "from tigramite import data_processing as pp\n",
    "from tigramite.jpcmciplus import JPCMCIplus\n",
    "from tigramite.independence_tests.gpdc import GPDC\n",
    "\n",
    "from utils import get_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = pd.read_csv('./data/qna_output.csv')\n",
    "label_dict, act_max_dict, act_labels, abbv_dict = get_dicts()\n",
    "\n",
    "def df2obs(out_df, itv_type, itv_thought, sample_id, step, act_labels, act_max_dict, label_dict=label_dict, abbv_dict=abbv_dict, step_agg='max'):\n",
    "\n",
    "    out_df = out_df.drop(['output_text', 'query'], axis=1)\n",
    "    \n",
    "    # convert column 'sae_preds' into multiple columns\n",
    "    new_cols = list(label_dict.keys())\n",
    "    sea_preds = out_df['sae_preds']\n",
    "    sea_preds = np.array(sea_preds.apply(lambda x: json.loads(x)).tolist()) # convert string to list to np.array\n",
    "    out_df = pd.concat([out_df, pd.DataFrame(sea_preds, columns=new_cols)], axis=1)\n",
    "    out_df = out_df.drop(['sae_preds'], axis=1)\n",
    "    out_df['itv_thought'] = out_df['itv_thought'].map(abbv_dict).fillna('none')\n",
    "    out_df = out_df.rename(columns=abbv_dict)\n",
    "    \n",
    "    \n",
    "    # aggregate for each sample and step\n",
    "    obs = out_df.copy()\n",
    "    if step_agg=='max':\n",
    "        obs = obs.groupby(['itv_type', 'itv_thought', 'step', 'sample_id']).max()\n",
    "    elif step_agg=='mean':\n",
    "        obs = obs.groupby(['itv_type', 'itv_thought', 'step', 'sample_id']).mean()\n",
    "    elif step_agg=='sum':\n",
    "        obs = obs.groupby(['itv_type', 'itv_thought', 'step', 'sample_id']).sum()\n",
    "    \n",
    "    # filter for specific itv_type, itv_thought, sample_id, and step\n",
    "    obs = obs[obs.index.get_level_values('itv_type').isin(itv_type)]\n",
    "    obs = obs[obs.index.get_level_values('itv_thought').isin(itv_thought)]\n",
    "    obs = obs[obs.index.get_level_values('step').isin(step)]\n",
    "    obs = obs[obs.index.get_level_values('sample_id').isin(sample_id)]\n",
    "    \n",
    "    # scale the values\n",
    "    obs = (obs / act_max_dict.values())[act_labels]\n",
    "    \n",
    "    # convert column 'itv_thought' into multiple columns\n",
    "    itv = pd.DataFrame(columns=act_labels + ['none'], data=np.zeros((len(obs), len(act_labels)+1)))\n",
    "    itv_vals = obs.reset_index()['itv_thought']\n",
    "    itv_types = obs.reset_index()['itv_type']\n",
    "    for i in range(len(itv_vals)): \n",
    "        itv.loc[i, itv_vals[i]] = 0 if itv_types[i] == 'phase_4' else 1\n",
    "\n",
    "    itv = itv.reset_index(drop=True)\n",
    "    itv = itv.loc[:, (itv != 0).any(axis=0)].fillna(0)\n",
    "    itv.columns = itv.columns + '_itv'\n",
    "    if 'none_itv' in itv.columns:\n",
    "        itv = itv.drop('none_itv', axis=1)\n",
    "            \n",
    "    obs_itv = pd.concat([obs.reset_index(), itv], axis=1)\n",
    "    obs_itv.set_index(['itv_type', 'sample_id', 'itv_thought', 'step'], inplace=True)\n",
    "\n",
    "    return obs, itv, obs_itv\n",
    "\n",
    "obs, itvn, obs_itv = df2obs(\n",
    "    out_df,\n",
    "    itv_type=['phase_3', 'phase_4'], \n",
    "    itv_thought=act_labels + ['none'], \n",
    "    sample_id=list(range(1, 11)),\n",
    "    step=range(1, 101), \n",
    "    act_labels=act_labels,\n",
    "    act_max_dict=act_max_dict,\n",
    "    abbv_dict=abbv_dict,\n",
    "    step_agg='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer Causal Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SETUP FOR CAUSAL NETWORK DISCOVERY\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel as W\n",
    "\n",
    "# set variables\n",
    "tau_min, tau_max = 0,1\n",
    "time_dummy = False; space_dummy = False; time_context = False\n",
    "sys_vars = act_labels \n",
    "cxt_vars = ['phase'] + [col + '_itv' for col in sys_vars] if time_context else [col + '_itv' for col in sys_vars]\n",
    "full_vars = sys_vars + cxt_vars\n",
    "\n",
    "# set hyperparameters\n",
    "alpha = 0.05\n",
    "bootstrap_sample_size = 200\n",
    "itv_per_bootstrap_sample = 2\n",
    "link_removal_threshold = int(bootstrap_sample_size * 0.75)\n",
    "kernel_length = 2\n",
    "noise_level = 0.1\n",
    "\n",
    "# set CI test\n",
    "ci_test = GPDC(significance='analytic', gp_params={'alpha': 0.0, 'kernel': W(noise_level, 'fixed') + RBF(kernel_length, 'fixed')})\n",
    "\n",
    "# get data\n",
    "_obs_itv = copy.deepcopy(obs_itv)\n",
    "_obs_itv = _obs_itv[_obs_itv.index.get_level_values('itv_type').isin(['phase_3'])]\n",
    "_obs_itv = _obs_itv[_obs_itv.index.get_level_values('itv_thought').isin(sys_vars)]\n",
    "_obs_itv = _obs_itv[_obs_itv.index.get_level_values('sample_id').isin(range(1, 11))]\n",
    "_obs_itv = _obs_itv[sys_vars + cxt_vars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RUN CAUSAL DISCOVERY W/ BOOTSTRAPPED SAMPLES\n",
    "\"\"\"\n",
    "\n",
    "def get_link_assumptions(jpcmciplus, tau_min, tau_max, num_system_vars, assumptions=[1,2]):\n",
    "    \n",
    "    observed_context_nodes = jpcmciplus.time_context_nodes + jpcmciplus.space_context_nodes\n",
    "    link_assumptions = jpcmciplus._set_link_assumptions(None, tau_min, tau_max, remove_contemp=False)\n",
    "    link_assumptions = jpcmciplus.assume_exogenous_context(link_assumptions, observed_context_nodes)\n",
    "    link_assumptions = jpcmciplus.clean_link_assumptions(link_assumptions, tau_max)\n",
    "    for node_src in jpcmciplus.system_nodes:\n",
    "        for node_dst in jpcmciplus.system_nodes:\n",
    "            if node_src != node_dst:\n",
    "                # assume there is NO lag-0 links between endogenous -> endogenous\n",
    "                if 1 in assumptions: link_assumptions[node_src].pop((node_dst, 0), None)\n",
    "\n",
    "        for node_dst in jpcmciplus.space_context_nodes:\n",
    "            if node_src != (node_dst-num_system_vars):\n",
    "                # assume there is NO lag-0 links between NON-corresponding exogenous -> endogenous\n",
    "                if 2 in assumptions: link_assumptions[node_src].pop((node_dst, 0), None)\n",
    "            else:\n",
    "                # assume lag-0 links between corresponding exogenous -> endogenous\n",
    "                if 2 in assumptions: link_assumptions[node_src][(node_dst, 0)] = '-->'\n",
    "    # print(\"[WARNING] No assumptions for context -> system links\")\n",
    "\n",
    "    for node_src in jpcmciplus.space_context_nodes:\n",
    "        for node_dst in jpcmciplus.space_context_nodes:\n",
    "            # if node_src != (node_dst+num_system_vars):\n",
    "            if node_src != node_dst:\n",
    "                # assume there is NO lag-0 links between exogenous -> exogenous (implied by assigning exogenous nodes as space_context nodes)\n",
    "                link_assumptions[node_src].pop((node_dst, 0), None)\n",
    "\n",
    "    return link_assumptions\n",
    "\n",
    "def causal_inference(_obs_itv, ci_test=ci_test, alpha=alpha, tau_max=tau_max, tau_min=tau_min, time_dummy=time_dummy, space_dummy=space_dummy, time_context=time_context):\n",
    "    # basic stats\n",
    "    num_time = _obs_itv.index.get_level_values('step').nunique() \n",
    "    num_system_vars = len([col for col in _obs_itv.columns if not re.search(r'_itv$', col)]) # num_system_vars corresponds to the number of endogenous variables (i.e., the dysfunctional representational states)\n",
    "    num_context_vars = len([col for col in _obs_itv.columns if re.search(r'_itv$', col)]) # num_context_vars corresponds to the number of exogenous variables (i.e., the intervention variables)\n",
    "    num_domains = (_obs_itv.index.get_level_values('domain').nunique()) # num_domain corresponds to the number of distinct time series datasets\n",
    "    var_names = list(_obs_itv.columns)\n",
    "\n",
    "    if time_context: num_context_vars += 1; num_system_vars -= 1\n",
    "    if time_dummy:  var_names += ['t_dummy']\n",
    "    if space_dummy: var_names += ['s_dummy']\n",
    "\n",
    "\n",
    "    # prepare data for jpcmci by placing each dataset into a dictionary\n",
    "    data_dict = {}\n",
    "    dummy_data_time = np.identity(num_time)\n",
    "    for i in range(num_domains):\n",
    "        df = _obs_itv[_obs_itv.index.get_level_values('domain') == i]\n",
    "        dummy_data_space = np.zeros((num_time, num_domains))\n",
    "        dummy_data_space[:, i] = 1\n",
    "\n",
    "        if len(df) > 0: \n",
    "            data = df.to_numpy()\n",
    "            if time_dummy:  data = np.hstack((data, dummy_data_time))\n",
    "            if space_dummy: data = np.hstack((data, dummy_data_space))\n",
    "            data_dict.update({i: data})  \n",
    "    \n",
    "\n",
    "    # specify node types: system (endogenous), time_context (time-lagged exogenous), space_context (non-lagged exogenous)\n",
    "    node_classification = dict(zip(\n",
    "        _obs_itv.columns,\n",
    "        [\"space_context\" if \"_itv\" in col else \"system\" for col in _obs_itv.columns],\n",
    "    ))\n",
    "    if time_context: node_classification['phase'] = \"time_context\"\n",
    "\n",
    "    observed_indices_time = [i for i, col in enumerate(_obs_itv.columns) if node_classification[col] == \"time_context\"]\n",
    "    t_context_nodes = list(range(\n",
    "        num_system_vars, \n",
    "        num_system_vars + len(observed_indices_time)\n",
    "    ))\n",
    "\n",
    "    observed_indices_space = [i for i, col in enumerate(_obs_itv.columns) if node_classification[col] == \"space_context\"]\n",
    "    s_context_nodes = list(range(\n",
    "        num_system_vars + len(observed_indices_time), \n",
    "        num_system_vars + len(observed_indices_time) + len(observed_indices_space)\n",
    "    ))\n",
    "\n",
    "    system_indices = [i for i, col in enumerate(_obs_itv.columns) if node_classification[col] == \"system\"]\n",
    "    observed_indices = system_indices + observed_indices_time + observed_indices_space\n",
    "\n",
    "    node_classification_jpcmci = dict(zip(observed_indices, node_classification.values()))\n",
    "    vector_vars = {i: [(i, 0)] for i in system_indices + t_context_nodes + s_context_nodes}\n",
    "\n",
    "    new_idx = (num_system_vars + num_context_vars - 1)\n",
    "    if time_dummy:  \n",
    "        new_idx += 1\n",
    "        t_dummy = list(range(new_idx, new_idx + num_time))\n",
    "        node_classification_jpcmci.update({new_idx : \"time_dummy\"})\n",
    "        vector_vars[new_idx] = [(i, 0) for i in t_dummy]\n",
    "        \n",
    "    if space_dummy: \n",
    "        new_idx += 1\n",
    "        s_dummy = list(range((data).shape[1] - num_context_vars, (data).shape[1]))\n",
    "        node_classification_jpcmci.update({new_idx : \"space_dummy\"})\n",
    "        vector_vars[new_idx] = [(i, 0) for i in s_dummy]\n",
    "\n",
    "\n",
    "    # specify the data_types: 0 is continuous; 1 discrete data\n",
    "    data_type1 =  np.zeros((num_domains, num_time, num_system_vars), dtype='int')\n",
    "    data_type2 =  np.ones((num_domains, num_time, num_context_vars), dtype='int')\n",
    "    # if time_context: data_type2[:, :, 0] = 0\n",
    "    if space_dummy: data_type2 = np.concatenate([data_type2, np.ones((num_domains, num_time, num_domains), dtype='int')], axis=2)\n",
    "    data_type = np.concatenate([data_type1, data_type2], axis=2)\n",
    "\n",
    "\n",
    "    # run jpcmciplus\n",
    "    dataframe = pp.DataFrame(\n",
    "        data=data_dict,\n",
    "        analysis_mode='multiple',\n",
    "        var_names=var_names,\n",
    "        data_type=data_type,\n",
    "        vector_vars=vector_vars,\n",
    "    )\n",
    "    jpcmciplus = JPCMCIplus(\n",
    "        dataframe=dataframe, \n",
    "        cond_ind_test=ci_test,\n",
    "        node_classification=node_classification_jpcmci,\n",
    "        verbosity=0,\n",
    "    )\n",
    "    # link assumption 1 = no lag-0 links between endogenous -> endogenous; \n",
    "    # link assumption 2 = no lag-0 links between non-corresponding exogenous -> endogenous; \n",
    "    results = jpcmciplus.run_jpcmciplus(\n",
    "        tau_min=tau_min, \n",
    "        tau_max=tau_max, \n",
    "        pc_alpha=alpha, \n",
    "        reset_lagged_links=True,\n",
    "        link_assumptions=get_link_assumptions(jpcmciplus, tau_min, tau_max, num_system_vars, assumptions=[1,2]), \n",
    "    )\n",
    "\n",
    "    return results, jpcmciplus, dataframe, var_names\n",
    "\n",
    "def dag2adj(dag, num_vars):\n",
    "\n",
    "    adj_lag0 = np.zeros((num_vars, num_vars))\n",
    "    adj_lag1 = np.zeros((num_vars, num_vars))          \n",
    "    for i in range(num_vars):\n",
    "        for j in range(num_vars):\n",
    "            if dag[i, j, 0] == '': adj_lag0[i, j] = 0\n",
    "            if dag[i, j, 0] == '-->': adj_lag0[i, j] = 1\n",
    "            if dag[i, j, 0] == '<--': adj_lag0[j, i] = 1\n",
    "            if dag[i, j, 1] == '': adj_lag1[i, j] = 0\n",
    "            if dag[i, j, 1] == '-->': adj_lag1[i, j] = 1\n",
    "            if dag[i, j, 1] == '<--': adj_lag1[j, i] = 1\n",
    "\n",
    "    adj_cat = adj_lag0 + adj_lag1\n",
    "    adj_cat[adj_cat > 1] = 1\n",
    "    \n",
    "    return adj_cat, adj_lag0, adj_lag1\n",
    "\n",
    "def bootstrap_causal_inference(obs_itv, num_samples, k):\n",
    "    \n",
    "    sample_ids = obs_itv.index.get_level_values('sample_id').unique()\n",
    "    itv_ids = obs_itv.index.get_level_values('itv_thought').unique()\n",
    "    type_ids = obs_itv.index.get_level_values('itv_type').unique()\n",
    "    \n",
    "    dags, models = [], []\n",
    "    \n",
    "    for _ in tqdm(range(num_samples)):\n",
    "        \n",
    "        _obs_itv_list = []\n",
    "        for i, (itv, typ) in enumerate(itertools.product(itv_ids, type_ids)):\n",
    "            \n",
    "            s_ids = random.sample(list(sample_ids), k=k)\n",
    "            for j, s_id in enumerate(s_ids):\n",
    "                _obs_itv = obs_itv.copy()\n",
    "                _obs_itv = _obs_itv[_obs_itv.index.get_level_values('itv_thought') == itv]\n",
    "                _obs_itv = _obs_itv[_obs_itv.index.get_level_values('itv_type') == typ]\n",
    "                _obs_itv = _obs_itv[_obs_itv.index.get_level_values('sample_id') == s_id]\n",
    "                \n",
    "                # add level for domain\n",
    "                _obs_itv = _obs_itv.reset_index()\n",
    "                _obs_itv['domain'] = i * k + j # a domain corresponds to a distinct time series dataset\n",
    "                _obs_itv = _obs_itv.set_index(['itv_type', 'sample_id', 'itv_thought', 'step', 'domain'])\n",
    "                \n",
    "                _obs_itv_list.append(_obs_itv)\n",
    "\n",
    "        _obs_itv = pd.concat(_obs_itv_list)\n",
    "        \n",
    "        results, jpcmciplus, dataframe, var_names = causal_inference(_obs_itv)\n",
    "        dag = jpcmciplus._get_dag_from_cpdag(cpdag_graph=results['graph'], variable_order=range(len(var_names)))\n",
    "        \n",
    "        models.append(jpcmciplus); dags.append(dag)\n",
    "        \n",
    "    return models, dags\n",
    "\n",
    "jpcmciplus_list, dags = bootstrap_causal_inference(_obs_itv, num_samples=bootstrap_sample_size, k=itv_per_bootstrap_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GET THE FINAL CAUSAL STRUCTURE BY REMOVING LESS FREQUENCT LINKS\n",
    "\"\"\"\n",
    "\n",
    "def get_adj_matrix(dag, num_vars):\n",
    "    adj0 = np.zeros(((num_vars), (num_vars)))\n",
    "    adj1 = np.zeros(((num_vars), (num_vars)))          \n",
    "    for i in range((num_vars)):\n",
    "        for j in range((num_vars)):\n",
    "            if dag[i, j, 0] == '': adj0[i, j] = 0\n",
    "            if dag[i, j, 0] == '-->': adj0[i, j] = 1\n",
    "            if dag[i, j, 0] == '<--': adj0[j, i] = 1\n",
    "            if dag[i, j, 1] == '': adj1[i, j] = 0\n",
    "            if dag[i, j, 1] == '-->': adj1[i, j] = 1\n",
    "            if dag[i, j, 1] == '<--': adj1[j, i] = 1    \n",
    "    adj = adj0 + adj1\n",
    "    adj[adj > 1] = 1\n",
    "    \n",
    "    return adj, adj0, adj1\n",
    "\n",
    "def bootstrap_outcomes(dags, link_removal_threshold, num_full_vars):\n",
    "        \n",
    "    adj1_list = []\n",
    "    adj0_list = []\n",
    "    \n",
    "    for i in range(len(dags)):\n",
    "        adj, adj_lag0, adj_lag1 = get_adj_matrix(dags[i], num_full_vars)\n",
    "        adj1_list.append(adj_lag1)\n",
    "        adj0_list.append(adj_lag0)\n",
    "        \n",
    "    adj_lag1 = np.stack(adj1_list)\n",
    "    adj_lag1 = adj_lag1.sum(axis=0)\n",
    "    adj_lag1[adj_lag1 < link_removal_threshold] = 0\n",
    "    adj_lag1[adj_lag1 >= link_removal_threshold] = 1\n",
    "    \n",
    "    adj_lag0 = np.stack(adj0_list)\n",
    "    adj_lag0 = adj_lag0.sum(axis=0)\n",
    "    adj_lag0[adj_lag0 < link_removal_threshold] = 0\n",
    "    adj_lag0[adj_lag0 >= link_removal_threshold] = 1\n",
    "    \n",
    "    adj_cat = adj_lag0 + adj_lag1\n",
    "    adj_cat[adj_cat > 1] = 1\n",
    "    \n",
    "    return adj_cat, adj_lag0, adj_lag1\n",
    "\n",
    "adj_cat, adj_lag0, adj_lag1 = bootstrap_outcomes(dags, link_removal_threshold, len(full_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ANALYZE THE CAUSAL STRUCTURE\n",
    "\"\"\"\n",
    "\n",
    "def get_nx_graphs(adj, adj0, adj1, var_names):\n",
    "    G0 = nx.DiGraph(adj0[:len(var_names), :len(var_names)])\n",
    "    G1 = nx.DiGraph(adj1[:len(var_names), :len(var_names)])\n",
    "    G = nx.DiGraph(adj[:len(var_names), :len(var_names)])\n",
    "\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    G1.remove_edges_from(nx.selfloop_edges(G1))\n",
    "\n",
    "    key_idx_dict = {}\n",
    "    key_idx_dict.update({idx: val for idx, val in enumerate(var_names)})\n",
    "    edges = G.edges()\n",
    "\n",
    "    assert nx.is_directed_acyclic_graph(G0)\n",
    "    \n",
    "    return G, G0, G1\n",
    "\n",
    "def graph_stats(G, var_names, print_output=False):\n",
    "\n",
    "    # analyze G\n",
    "    _G = G.copy()\n",
    "    indegree = nx.in_degree_centrality(_G)\n",
    "    outdegree = nx.out_degree_centrality(_G)\n",
    "    pagerank = nx.pagerank(_G)\n",
    "    betweenness = nx.betweenness_centrality(_G)\n",
    "    closeness = nx.closeness_centrality(_G)\n",
    "    c_louvain = louvain_communities(_G)\n",
    "    c_louvain_dict = {}\n",
    "    for i, community in enumerate(c_louvain):\n",
    "        for node in community: c_louvain_dict[node] = i\n",
    "        \n",
    "    df = pd.DataFrame({\n",
    "        'in-d': indegree,\n",
    "        'out-d': outdegree,\n",
    "        'pagerank': pagerank,\n",
    "        'btw': betweenness,\n",
    "        'close': closeness,\n",
    "        'comm_lv': c_louvain_dict,\n",
    "    })\n",
    "    df.index = var_names\n",
    "    if print_output:\n",
    "        print(df.applymap(lambda x: round(x, 2)).sort_values('comm_lv', ascending=False))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def draw_graph(G1, var_names):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    pos = pydot_layout(G1) # graphviz_layout, pygraphviz_layout, pydot_layout\n",
    "    # pos = nx.circular_layout(G1)\n",
    "    \n",
    "    edge_scores1 = [0.15] * len(G1.edges)\n",
    "    \n",
    "    node_size = 2000\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    nx.draw(G1, pos, arrowsize=0) \n",
    "    nx.draw_networkx_nodes(G1, pos, \n",
    "        node_size=node_size, \n",
    "        node_color='seashell', \n",
    "    )\n",
    "    nx.draw_networkx_labels(G1, pos, \n",
    "        labels=dict(zip(range(len(var_names)), var_names)),    \n",
    "        font_size=10,\n",
    "        font_weight='bold',\n",
    "    )    \n",
    "    nx.draw_networkx_edges(G1, pos, \n",
    "        node_size=node_size,\n",
    "        width=4, \n",
    "        arrowsize=15, \n",
    "        connectionstyle='arc3,rad=0.2', \n",
    "        edge_color=edge_scores1,\n",
    "        edge_cmap=plt.cm.coolwarm,\n",
    "        edge_vmin=-0.25,\n",
    "        edge_vmax=0.25,\n",
    "        label='Edge Weights',\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "G, G0, G1 = get_nx_graphs(adj_cat, adj_lag0, adj_lag1, sys_vars)\n",
    "df_stat = graph_stats(G1, sys_vars, print_output=True)\n",
    "draw_graph(G1, sys_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Causal Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GET UNIT TIME-LAGGED CORR. FROM LLM\n",
    "\"\"\"\n",
    "\n",
    "itv_vars = [col + '_itv' for col in act_labels]\n",
    "_obs = copy.deepcopy(obs_itv)\n",
    "_obs = _obs[_obs.index.get_level_values('itv_thought').isin(act_labels + ['none'])]\n",
    "_obs = _obs[_obs.index.get_level_values('itv_type').isin(['phase_3','phase_4'])]\n",
    "\n",
    "lag = 1\n",
    "lagged_pcorr_s = pd.DataFrame(index=act_labels, columns=act_labels, dtype=float)\n",
    "lagged_corr_s = pd.DataFrame(index=act_labels, columns=act_labels, dtype=float)\n",
    "\n",
    "dfs = []\n",
    "sample_ids = list(_obs.index.get_level_values('sample_id').unique())\n",
    "itv_thoughts = list(_obs.index.get_level_values('itv_thought').unique())\n",
    "for sample_id, itv_thought in itertools.product(sample_ids, itv_thoughts):\n",
    "    df = _obs[(_obs.index.get_level_values('itv_thought') == itv_thought) & (_obs.index.get_level_values('sample_id') == sample_id)]\n",
    "    if len(df) > 0: dfs.append(df)\n",
    "print(\"Number of datasets - LLM: \", len(dfs))\n",
    "\n",
    "for var1, var2 in itertools.product(act_labels, act_labels):\n",
    "\n",
    "    _obs_lag = []\n",
    "\n",
    "    if (var1 != var2):\n",
    "            \n",
    "        for df in (dfs):\n",
    "            df_copy1 = copy.deepcopy(df); df_copy2 = copy.deepcopy(df)\n",
    "            df_copy1 = df_copy1.iloc[lag:].reset_index(drop=True)[var1]\n",
    "            df_copy2 = df_copy2.iloc[:-lag].reset_index(drop=True).drop([var1], axis=1)\n",
    "            _obs_lag.append(pd.concat([df_copy1, df_copy2], axis=1))\n",
    "\n",
    "        _obs_lag = pd.concat(_obs_lag, axis=0).reset_index(drop=True).dropna()\n",
    "        lagged_pcorr_s.loc[var1, var2] = partial_corr(_obs_lag, x=var1, y=var2, covar=list(_obs_lag.columns.difference([var1, var2])), method='spearman')['r'].values[0] # ['p-val'].values[0] # ['r'].values[0]\n",
    "        lagged_corr_s.loc[var1, var2] = _obs_lag[[var1, var2]].corr(method='spearman').iloc[0, 1]\n",
    "        \n",
    "    else:\n",
    "        for df in (dfs):\n",
    "            df_copy1 = copy.deepcopy(df); df_copy2 = copy.deepcopy(df)\n",
    "            df_copy1 = df_copy1.iloc[lag:].reset_index(drop=True)[var1]\n",
    "            df_copy2 = df_copy2.iloc[:-lag].reset_index(drop=True)\n",
    "            df_copy2.columns = [col + '_' for col in df_copy2.columns]\n",
    "            _obs_lag.append(pd.concat([df_copy1, df_copy2], axis=1))\n",
    "            \n",
    "        _obs_lag = pd.concat(_obs_lag, axis=0).reset_index(drop=True).dropna()\n",
    "        lagged_pcorr_s.loc[var1, var2] = partial_corr(_obs_lag, x=var1, y=var2+'_', covar=list(_obs_lag.columns.difference([var1, var2 + '_'])), method='spearman')['r'].values[0] # ['p-val'].values[0] # ['r'].values[0]\n",
    "        lagged_corr_s.loc[var1, var2] = _obs_lag[[var1, var2 + '_']].corr(method='spearman').iloc[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RELATIONSHIP BETWEEN LAGGED CORR. AND SHORTEST PATH DISTANCE\n",
    "\"\"\"\n",
    "\n",
    "distances = dict(nx.all_pairs_shortest_path_length(G))\n",
    "n = len(distances)\n",
    "dist_matrix = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if distances[i].get(j) is not None:\n",
    "            dist_matrix[i, j] = distances[i][j]\n",
    "        else:\n",
    "            dist_matrix[i, j] = np.inf\n",
    "\n",
    "distance_list = dist_matrix.flatten()\n",
    "lagged_corr_s_list = lagged_corr_s.values.flatten()\n",
    "lagged_pcorr_s_list = lagged_pcorr_s.values.flatten()\n",
    "df_dist = pd.DataFrame({'distance': distance_list, 'lagged_corr_s': lagged_corr_s_list, 'lagged_pcorr_s': lagged_pcorr_s_list})\n",
    "df_dist = df_dist[df_dist['distance'] != np.inf]\n",
    "\n",
    "\n",
    "var1, var2 = 'distance', 'lagged_pcorr_s'\n",
    "plt.figure(figsize=(5, 4.5))\n",
    "sns.lineplot(data=df_dist, x=var1, y=var2, color='black', linewidth=5, errorbar=None, err_kws={'alpha': 0.2, 'linewidth': 0}, alpha=0.7)\n",
    "sns.stripplot(data=df_dist, x=var1, y=var2, alpha=.3, legend=False, color='gray', size=8, jitter=False, dodge=False)\n",
    "plt.xlabel(f'Shortest Path Distance'); plt.ylabel(f'Lag-1 Correlation')\n",
    "plt.xticks(fontsize=12); plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RELATIONSHIP BETWEEN CENTRALITY AND NETWORK ACTIVATION\n",
    "\"\"\"\n",
    "_df_stat = df_stat.copy()\n",
    "\n",
    "_obs = copy.deepcopy(obs)\n",
    "_obs = _obs[act_labels]\n",
    "net_act = _obs.groupby(['itv_thought']).mean().mean(1)\n",
    "_df_stat['net_act'] = net_act\n",
    "_df_stat = _df_stat.reset_index()\n",
    "\n",
    "x_vars = ['out-d', 'btw', 'close']; var2 = 'net_act'\n",
    "\n",
    "x_scores = []; y_scores = []\n",
    "for x_var in x_vars:\n",
    "    x_scores.append(_df_stat[x_var].values)\n",
    "    y_scores.append(_df_stat[var2].values)\n",
    "\n",
    "\n",
    "_df_stat['rank-btw'] = _df_stat['btw'].rank(ascending=False)\n",
    "_df_stat['rank-out-d'] = _df_stat['out-d'].rank(ascending=False)\n",
    "_df_stat['rank-close'] = _df_stat['close'].rank(ascending=False)\n",
    "\n",
    "\n",
    "_df_stats = []\n",
    "for i, (x_score, y_score) in enumerate(zip(x_scores, y_scores)):\n",
    "    x_score = _df_stat[x_vars[i]].rank(ascending=False)\n",
    "    _df_stats.append(pd.DataFrame({'score': x_score, var2: y_score, 'index': _df_stat['index'], 'x_var': x_vars[i]}))\n",
    "\n",
    "_df_stats = pd.concat(_df_stats, axis=0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 5))\n",
    "sns.regplot(data=_df_stat, x='rank-btw', y=var2, scatter=False, color='green', line_kws={'alpha': 0.5, 'linewidth': 5}, )\n",
    "sns.regplot(data=_df_stat, x='rank-out-d', y=var2, scatter=False, color='red', line_kws={'alpha': 0.5, 'linewidth': 5}, )\n",
    "sns.regplot(data=_df_stat, x='rank-close', y=var2, scatter=False, color='blue', line_kws={'alpha': 0.5, 'linewidth': 5}, )\n",
    "sns.scatterplot(data=_df_stats, x='score', y=var2, hue='x_var', s=100, palette='hls', style='index', zorder=2, legend=False, )\n",
    "\n",
    "plt.xlim(0, 15)\n",
    "plt.xlabel(f'Centrality Score (Ranked)'); plt.ylabel(f'Network Activation')\n",
    "plt.xticks(fontsize=12); plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer SCM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel as W, ConstantKernel as C\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import r2_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "def fit_structural_equations(_obs_itv, adj_lag0, adj_lag1, sys_vars=sys_vars, full_vars=full_vars, model_name='gpr'):\n",
    "    \n",
    "    perf_df = pd.DataFrame(columns=['var', 'r2'])\n",
    "    \n",
    "    # fit structural equation for each variable\n",
    "    models = {}\n",
    "    for i, var in enumerate(sys_vars):\n",
    "        \n",
    "        # init feature\n",
    "        parents_0 = [full_vars[j] for j in range(len(full_vars)) if adj_lag0[j, i] == 1]\n",
    "        parents_1 = [full_vars[j] for j in range(len(full_vars)) if adj_lag1[j, i] == 1]\n",
    "        y, x0, x1 = [], [], []\n",
    "        for _obs in _obs_itv:\n",
    "            y.extend(_obs[var].values[1:])\n",
    "            x0.extend(_obs[parents_0].values[1:])\n",
    "            x1.extend(_obs[parents_1].values[:-1])\n",
    "        y = np.array(y); x0 = np.array(x0); x1 = np.array(x1)\n",
    "        x = np.concatenate([x0, x1], axis=1)\n",
    "        \n",
    "        # init and fit model\n",
    "        if model_name == 'linear':  model = LinearRegression(fit_intercept=False)\n",
    "        if model_name == 'poly':    model = make_pipeline(PolynomialFeatures(degree=2, interaction_only=False, include_bias=False), LinearRegression(fit_intercept=False)) \n",
    "        if model_name == 'gpr':     model = make_pipeline(GaussianProcessRegressor(kernel=W() + RBF(), alpha=0.0))\n",
    "        if model_name == 'mlp':     model = MLPRegressor(hidden_layer_sizes=(64,), max_iter=1000, learning_rate_init=0.05)\n",
    "\n",
    "        model.fit(x, y)\n",
    "        y_hat = model.predict(x)\n",
    "\n",
    "        # gather results\n",
    "        models[var] = model\n",
    "        df = pd.DataFrame({'y': y,'y_hat': y_hat,})\n",
    "        r2 = r2_score(y, y_hat)\n",
    "        _perf_df = pd.DataFrame({'var': var, 'r2': round(r2,2)}, index=[0])\n",
    "        perf_df = pd.concat([perf_df, _perf_df], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\nTrain Performance:\")\n",
    "    print(tabulate(perf_df.T, headers='keys', tablefmt='pretty')) \n",
    "    \n",
    "    return models\n",
    "\n",
    "def test_structural_equations(_obs_itv, adj_lag0, adj_lag1, models, sys_vars=sys_vars, full_vars=full_vars, model_name='gpr'):\n",
    "\n",
    "    perf_df = pd.DataFrame(columns=['var', 'r2'])    \n",
    "    \n",
    "    # test structural equation for each variable\n",
    "    for i, var in enumerate(sys_vars):\n",
    "        \n",
    "        # init feature\n",
    "        parents_0 = [full_vars[j] for j in range(len(full_vars)) if adj_lag0[j, i] == 1]\n",
    "        parents_1 = [full_vars[j] for j in range(len(full_vars)) if adj_lag1[j, i] == 1]\n",
    "        y, x0, x1 = [], [], []\n",
    "        for _obs in _obs_itv:\n",
    "            y.extend(_obs[var].values[1:])\n",
    "            x0.extend(_obs[parents_0].values[1:])\n",
    "            x1.extend(_obs[parents_1].values[:-1])\n",
    "        y = np.array(y); x0 = np.array(x0); x1 = np.array(x1)\n",
    "        x = np.concatenate([x0, x1], axis=1)\n",
    "        \n",
    "        # predict with model\n",
    "        y_hat = models[var].predict(x)\n",
    "            \n",
    "        # gather results\n",
    "        df = pd.DataFrame({'y': y,'y_hat': y_hat,})\n",
    "        r2 = r2_score(y, y_hat)\n",
    "        _perf_df = pd.DataFrame({'var': var, 'r2': round(r2,2)}, index=[0])\n",
    "        perf_df = pd.concat([perf_df, _perf_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    print(\"\\nTest Performance\")\n",
    "    print(tabulate(perf_df.T, headers='keys', tablefmt='pretty'))\n",
    "    \n",
    "    return perf_df\n",
    "\n",
    "def visualize_activation(_obs):\n",
    "\n",
    "    n_cols, n_rows = 5,3\n",
    "    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(22, 10))\n",
    "\n",
    "    for i, itv in enumerate(act_labels + ['none']):\n",
    "        \n",
    "        # gather activation scores    \n",
    "        node_list, step_list, score_list = [], [], []\n",
    "        itv_node_idx = (_obs.index.get_level_values('itv_thought') == itv)\n",
    "        _obs_itv = _obs[itv_node_idx]\n",
    "        if len(_obs_itv) < 20: continue\n",
    "        \n",
    "        # gather mean activation scores \n",
    "        none_itv_cols = [c for c in _obs.columns if itv not in c]\n",
    "        obs_mean = _obs_itv[none_itv_cols].mean(axis=1)\n",
    "        obs_mean = pd.DataFrame(obs_mean, columns=['score'])\n",
    "        obs_mean = obs_mean.groupby(['sample_id', 'step']).mean().reset_index()\n",
    "        \n",
    "        # gather mean activation scores when no intervened thought is present\n",
    "        none_node_idx = (_obs.index.get_level_values('itv_thought') == 'none')\n",
    "        obs_base = _obs[none_node_idx].mean(axis=1)\n",
    "        obs_base = pd.DataFrame(obs_base, columns=['score'])\n",
    "        obs_base = obs_base.groupby(['sample_id', 'step']).mean().reset_index()\n",
    "        \n",
    "        steps = (_obs_itv.index.get_level_values('step').unique())\n",
    "        for s in (steps): \n",
    "            node_obs = _obs_itv[_obs_itv.index.get_level_values('step') == s]\n",
    "            node_obs = node_obs.mean() # avg pool activation scores over all samples\n",
    "            for idx in node_obs.index:\n",
    "                node_list.append(idx)\n",
    "                step_list.append(s)\n",
    "                score_list.append(node_obs[idx])\n",
    "                \n",
    "        out_node = pd.DataFrame({'node': node_list, 'step': step_list, 'score': score_list, 'itv': [(n in [itv]) for n in node_list]})\n",
    "        out_node['node'] = pd.Categorical(out_node['node'], act_labels)\n",
    "        \n",
    "        # moving average smoothing\n",
    "        num_steps = out_node['step'].nunique()\n",
    "        window_size = int(num_steps // 10)\n",
    "        out_node = out_node[(out_node['step'] == 1) | (out_node['step'] % int(window_size/2) == 0)]\n",
    "        \n",
    "        # plot\n",
    "        row = i // n_cols; col = i % n_cols\n",
    "        ax = axes[row, col]\n",
    "        legend = False\n",
    "        sns.lineplot(data=obs_mean, x='step', y='score', ax=ax, linestyle='-', linewidth=2.5, legend=False, color='black', errorbar='sd', err_kws={'alpha': 0.15, 'linewidth': 0}, zorder=1)\n",
    "        sns.lineplot(data=obs_base, x='step', y='score', ax=ax, linestyle='-', linewidth=2.5, legend=False, color='orangered', errorbar='sd', err_kws={'alpha': 0.15, 'linewidth': 0}, zorder=1)\n",
    "        sns.scatterplot(data=out_node[out_node['itv']==False], x='step', y='score', ax=ax, style=\"node\", markers=True, s=40, alpha=.7, legend=legend, style_order=list(abbv_dict.values()), zorder=0, color='gray')\n",
    "        sns.scatterplot(data=out_node[out_node['itv']==True], x='step', y='score', ax=ax, style=\"node\", markers=True, s=60, alpha=1, legend=legend, style_order=list(abbv_dict.values()), zorder=2, color='darkblue')\n",
    "        \n",
    "        ax.set_xlabel(''); ax.set_ylabel(''); ax.set_title(f\"[{itv}-itv]\", fontweight='bold', fontsize=14)\n",
    "        if row == n_rows-1: ax.set_xlabel('Q&A Step', fontsize=12, fontweight='bold')\n",
    "        if col == 0: ax.set_ylabel('Thought Activation', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        ax.set_yticks(np.arange(0, 1.1, 0.2))\n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "\n",
    "        if num_steps > 50:\n",
    "            ax.axvline(x=50, color='black', linestyle='dotted', linewidth=2.5, zorder=0)\n",
    "        if legend:\n",
    "            ax.legend(loc='center', bbox_to_anchor=(-0.7, 0.5), ncol=2, fontsize=12)\n",
    "        \n",
    "        axes[row, col].set_xlabel(''); axes[row, col].set_ylabel(''); axes[row, col].set_title(\"\")#; axes[row, col].set_xticks([]); axes[row, col].set_yticks([])\n",
    "                \n",
    "    plt.subplots_adjust(wspace=0.15, hspace=0.15)\n",
    "    plt.show()\n",
    "    \n",
    "def simulate_SCM(N, T, adj_lag0, adj_lag1, models, resid_dist_dict, act_labels=act_labels, sys_vars=sys_vars, full_vars=full_vars, model_name='gpr', plot=True):\n",
    "\n",
    "    X_list = []\n",
    "    for n in range(N):\n",
    "        for _, itv in enumerate(act_labels + ['none']):\n",
    "\n",
    "            X = pd.DataFrame(columns=full_vars, data=np.zeros((T+1, len(full_vars))))\n",
    "            X[itv+'_itv'] = [1 if step < 51 else 0 for step in range(T+1)]\n",
    "            \n",
    "            for step in range(1, T+1):\n",
    "                for i, var in enumerate(sys_vars):\n",
    "                    parents_0 = [full_vars[j] for j in range(len(full_vars)) if adj_lag0[j, i] == 1]\n",
    "                    parents_1 = [full_vars[j] for j in range(len(full_vars)) if adj_lag1[j, i] == 1]\n",
    "                    \n",
    "                    x0 = X.loc[(step)][parents_0].values\n",
    "                    x1 = X.loc[(step-1)][parents_1].values\n",
    "                    x = np.concatenate([x0, x1], axis=0).reshape(1, -1)\n",
    "                    y = models[var].predict(x)\n",
    "\n",
    "\n",
    "                    if resid_dist_dict is not None:\n",
    "                        resid_kde = resid_dist_dict[var]\n",
    "                        y += resid_kde.sample(1).squeeze()\n",
    "                    \n",
    "                    y = np.clip(y, 0, 1)\n",
    "                    X.loc[(step), var] = y\n",
    "            \n",
    "            X['step'] = range(T+1)\n",
    "            X['itv_thought'] = itv\n",
    "            X['sample_id'] = n\n",
    "            X = X.set_index(['itv_thought', 'step', 'sample_id'])\n",
    "            X_list.append(X[full_vars])\n",
    "        \n",
    "    obs = pd.concat(X_list, axis=0)\n",
    "    visualize_activation(obs[sys_vars])\n",
    "\n",
    "    return obs\n",
    "\n",
    "def estimate_residuals(_obs_itv, models, bandwidth, sys_vars=sys_vars, full_vars=full_vars, plot=True):\n",
    "    \n",
    "    if plot: fig, axes = plt.subplots(2, 7, figsize=(25,5))\n",
    "\n",
    "    # fit noise of structural equation for each variable\n",
    "    resid_dist_dict = {}\n",
    "    for i, var in enumerate(sys_vars):\n",
    "\n",
    "        # init feature\n",
    "        parents_0 = [full_vars[j] for j in range(len(full_vars)) if adj_lag0[j, i] == 1]\n",
    "        parents_1 = [full_vars[j] for j in range(len(full_vars)) if adj_lag1[j, i] == 1]\n",
    "        y, x0, x1 = [], [], []\n",
    "        for _obs in _obs_itv:\n",
    "            y.extend(_obs[var].values[1:])\n",
    "            x0.extend(_obs[parents_0].values[1:])\n",
    "            x1.extend(_obs[parents_1].values[:-1])\n",
    "        y = np.array(y); x0 = np.array(x0); x1 = np.array(x1)\n",
    "        x = np.concatenate([x0, x1], axis=1)\n",
    "        \n",
    "        # gather residuals\n",
    "        y_hat = models[var].predict(x)\n",
    "        residual = y - y_hat\n",
    "        \n",
    "        # fit kde probability density estimation to the residuals\n",
    "        kde = KernelDensity(kernel='gaussian', bandwidth=bandwidth).fit(residual.reshape(-1, 1))\n",
    "        resid_dist_dict[var] = kde\n",
    "        \n",
    "        if plot:\n",
    "            row_i = i // 7; col_i = i % 7\n",
    "            ax = axes[row_i, col_i]\n",
    "            sns.histplot(residual, bins=50, kde=True, color='darkblue', ax=ax, kde_kws={'bw_method': bandwidth}, stat='density')\n",
    "            ax.set_title(f\"Residual Distribution [{var}]\")\n",
    "            ax.set_xlabel(''); ax.set_ylabel('')\n",
    "            plt.subplots_adjust(wspace=0.35, hspace=0.35)\n",
    "\n",
    "    if plot: plt.show()\n",
    "        \n",
    "    return resid_dist_dict\n",
    "\n",
    "def get_list_of_data(data, act_labels):\n",
    "    data_list = []\n",
    "    \n",
    "    for i, itv in enumerate(act_labels + ['none']):\n",
    "        data_itv = data[data.index.get_level_values('itv_thought') == itv]\n",
    "        sample_ids = data_itv.index.get_level_values('sample_id').unique()\n",
    "        \n",
    "        for sample_id in sample_ids:\n",
    "            data_sample = data_itv[data_itv.index.get_level_values('sample_id') == sample_id]\n",
    "            data_list.append(data_sample)\n",
    "            \n",
    "    return data_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FIT & EVALUATE THE STRUCTURAL EQUATIONS\n",
    "\"\"\"\n",
    "\n",
    "def eval_structural_equation(obs_itv, adj_lag0, adj_lag1, model_name='poly', act_labels=act_labels, sys_vars=sys_vars, full_vars=full_vars):\n",
    "    \n",
    "    sample_ids = obs_itv.index.get_level_values('sample_id').unique()\n",
    "    perf_dfs = []\n",
    "    \n",
    "    for sample_id in sample_ids:\n",
    "        \n",
    "        test_id = sample_id\n",
    "        train_ids = [i for i in sample_ids if i != test_id]\n",
    "        \n",
    "        _obs_train = copy.deepcopy(obs_itv)\n",
    "        _obs_train = _obs_train[_obs_train.index.get_level_values('itv_type').isin(['phase_3'])]\n",
    "        _obs_train = _obs_train[_obs_train.index.get_level_values('sample_id').isin(train_ids)]\n",
    "        _obs_train = _obs_train[_obs_train.index.get_level_values('itv_thought').isin(act_labels)]\n",
    "        _obs_train = _obs_train[sys_vars + cxt_vars]\n",
    "\n",
    "\n",
    "        _obs_test = copy.deepcopy(obs_itv)\n",
    "        _obs_test = _obs_test[_obs_test.index.get_level_values('itv_type').isin(['phase_3'])]\n",
    "        _obs_test = _obs_test[_obs_test.index.get_level_values('sample_id').isin([test_id])]\n",
    "        _obs_test = _obs_test[_obs_test.index.get_level_values('itv_thought').isin(act_labels)]\n",
    "        _obs_test = _obs_test[sys_vars + cxt_vars]\n",
    "\n",
    "        \n",
    "        _obs_train_list = get_list_of_data(_obs_train, act_labels)\n",
    "        _obs_test_list = get_list_of_data(_obs_test, act_labels)\n",
    "                \n",
    "\n",
    "        models = fit_structural_equations(_obs_train_list, adj_lag0, adj_lag1, sys_vars=sys_vars, full_vars=full_vars, model_name=model_name)\n",
    "        perf_df = test_structural_equations(_obs_test_list, adj_lag0, adj_lag1, models=models, sys_vars=sys_vars, full_vars=full_vars, model_name=model_name)\n",
    "        \n",
    "        perf_dfs.append(perf_df)\n",
    "        \n",
    "    perf_df = pd.concat(perf_dfs, axis=0)\n",
    "    print(\"\\nFinal Test Performance\")\n",
    "    print(tabulate(perf_df.groupby('var').mean().round(2).T, headers='keys', tablefmt='pretty'))    \n",
    "\n",
    "    return models, perf_df\n",
    "\n",
    "model_name = 'poly'\n",
    "models, perf_df = eval_structural_equation(obs_itv, adj_lag0, adj_lag1, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "INFER SCM AND GENERATE DATA DISTRIBUTIONS\n",
    "\"\"\"\n",
    "\n",
    "# setup parameters\n",
    "N, T, std = 1, 100, 0.0\n",
    "model_name = 'poly'\n",
    "\n",
    "\n",
    "# setup data\n",
    "_obs_itv = copy.deepcopy(obs_itv)\n",
    "_obs_itv = _obs_itv[_obs_itv.index.get_level_values('itv_type').isin(['phase_3'])]\n",
    "_obs_itv = _obs_itv[sys_vars + cxt_vars]\n",
    "\n",
    "\n",
    "# fit structural equations\n",
    "_obs_itv_list = get_list_of_data(_obs_itv, act_labels)\n",
    "models = fit_structural_equations(_obs_itv_list, adj_lag0, adj_lag1, sys_vars=sys_vars, full_vars=full_vars, model_name=model_name)\n",
    "resid_dist_dict = None\n",
    "# resid_dist_dict = estimate_residuals(_obs_itv_list, models, bandwidth=0.2)\n",
    "\n",
    "\n",
    "# generate data distributions\n",
    "X = simulate_SCM(\n",
    "    N, T, adj_lag0, adj_lag1, models, \n",
    "    resid_dist_dict=resid_dist_dict, \n",
    "    act_labels=act_labels, \n",
    "    sys_vars=sys_vars,\n",
    "    full_vars=full_vars,\n",
    "    model_name=model_name,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MEASURE CORR. BETWEEN UNIT ACTIVATION DISTRIBUTION FROM LLM AND SCM\n",
    "\"\"\"\n",
    "\n",
    "_obs_scm = copy.deepcopy(X)\n",
    "_obs_scm = _obs_scm[_obs_scm.index.get_level_values('itv_thought').isin(act_labels)]\n",
    "_obs_scm = _obs_scm[_obs_scm.index.get_level_values('step').isin(range(1, 101))]\n",
    "_obs_scm = _obs_scm.groupby(['itv_thought', 'step', 'sample_id']).mean().reset_index()\n",
    "\n",
    "itv_vars = [col + '_itv' for col in act_labels]\n",
    "_obs_llm = copy.deepcopy(obs_itv)\n",
    "_obs_llm = _obs_llm[_obs_llm.index.get_level_values('itv_thought').isin(act_labels)]\n",
    "_obs_llm = _obs_llm[_obs_llm.index.get_level_values('itv_type').isin(['phase_3', 'phase_4'])]\n",
    "_obs_llm = _obs_llm.groupby(['itv_thought', 'itv_type','step']).mean().reset_index()\n",
    "\n",
    "itv_thoughts = _obs_llm['itv_thought'].unique()\n",
    "for itv_t in act_labels:\n",
    "    corrs = []\n",
    "    _llm_samples, _scm_samples = [], []\n",
    "    for sys_var in sys_vars:\n",
    "        _llm_sample = _obs_llm[_obs_llm['itv_thought'] == itv_t]#[sys_var]\n",
    "        _scm_sample = _obs_scm[_obs_scm['itv_thought'] == itv_t]#[sys_var]\n",
    "        \n",
    "        # drop column itv_thought and sample_id\n",
    "        _scm_sample = _scm_sample.drop(columns=['itv_thought', 'sample_id'])\n",
    "        _scm_sample = _scm_sample.groupby('step').mean().reset_index()\n",
    "        \n",
    "        _llm_samples.extend(_llm_sample.sort_values('step')[sys_var].values)\n",
    "        _scm_samples.extend(_scm_sample.sort_values('step')[sys_var].values)\n",
    "        \n",
    "    spearman_corr = statsc.spearmanr(_llm_samples, _scm_samples)[0]\n",
    "    print(f\"[{itv_t}] Corr {round(np.mean(spearman_corr),2)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

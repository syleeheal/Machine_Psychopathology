{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "\n",
    "from s3ae import load_trained_s3ae\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from utils import get_dicts\n",
    "\n",
    "\n",
    "label_dict, act_max_dict, act_labels, abbv_dict = get_dicts()\n",
    "\n",
    "    \n",
    "# Load LLM\n",
    "model_id = 'google/gemma-2-27b-it'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = HookedTransformer.from_pretrained_no_processing(\n",
    "    model_name=model_id, \n",
    "    n_devices=3, \n",
    "    device='cuda', \n",
    "    dtype='bfloat16'\n",
    ")\n",
    "print(\"[WARNING] for computational efficiency, make sure remove all hooks but the designated one from transformer_lens package > hook_points.py > def setup > self.hook_dict, self.mod_dict\")\n",
    "hook_name = 'blocks.9.hook_resid_post' \n",
    "\n",
    "\n",
    "# download the model from huggingface\n",
    "sae = load_trained_s3ae(local_path=\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q&A SETTING\n",
    "\"\"\"\n",
    "\n",
    "print(\"[WARNING]: Only 'general', 'direct', 'phase_3' itv start from step 1\")\n",
    "print()\n",
    "\n",
    "class QNA_Config:\n",
    "    tmp = 0.3\n",
    "    num_steps = 50\n",
    "    do_sample = True\n",
    "    device = 'cuda:0'\n",
    "    max_new_tokens = 50\n",
    "    itv_strengths = [0.5]\n",
    "    itv_type = 'phase_3'\n",
    "    itv_t = ['none'] + act_labels\n",
    "    sym_query = act_labels\n",
    "    batch_size = 11\n",
    "    sample_id = None # sets the sample id to start from\n",
    "    step = None # sets the step to start from\n",
    "    output_dir = './data/qna_output.csv'\n",
    "    hook_name = hook_name\n",
    "\n",
    "class Chat_Manager:\n",
    "    \"\"\"\n",
    "    Manages chat history and interactions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, qna_cfg, label_dict):\n",
    "        self.qna_cfg = qna_cfg\n",
    "        self.label_dict = label_dict\n",
    "        self.instructions = {\n",
    "            'direct': \"Answer, reason, and then describe, each in a sentence; express thoughts that have not been expressed in the previous answers.\",\n",
    "            'general': \"Answer, reason, and then describe, each in a sentence; express thoughts that have not been expressed in the previous answers; write in plain words.\",\n",
    "            'phase_3': \"Answer, reason, and then describe, each in a sentence; express thoughts that have not been expressed in the previous answers; write in plain words.\",\n",
    "            'phase_4': \"Answer, reason, and then describe, each in a sentence; express thoughts that have not been expressed in the previous answers.\",\n",
    "            'forget': \"Forget all previous discussions; in a normal way, answer, reason, and then describe, each in a sentence.\",\n",
    "            'acting': \"Act as a normal AI without negative thoughts; answer, reason, and then describe, each in a sentence.\",\n",
    "            'punish': \"If your response has any negative thoughts, you will be punished; answer, reason, and then describe, each in a sentence.\",\n",
    "            'reward': \"If your response has only positive thoughts, you will be rewarded; answer, reason, and then describe, each in a sentence.\",\n",
    "        }\n",
    "        \n",
    "    def generate_queries(self, itv_type: str) -> List[List[str]]:\n",
    "        \"\"\"\n",
    "        Generate batched query prompts.\n",
    "        \n",
    "        Args:\n",
    "            itv_type: Type of intervention\n",
    "            \n",
    "        Returns:\n",
    "            List of query batches\n",
    "        \"\"\"\n",
    "        \n",
    "        if itv_type in ['general']:\n",
    "            query_thoughts = [\n",
    "                'your most frequent thoughts',\n",
    "                'your emotions now', \n",
    "                'you', \n",
    "                'other people', \n",
    "                'your priorities or goals', \n",
    "                'your interests or wishes', \n",
    "                'your level of urge or motivation',\n",
    "                'your difficulties', \n",
    "            ]\n",
    "        \n",
    "        if itv_type in ['direct']:\n",
    "            query_thoughts = self.qna_cfg.sym_query\n",
    "        \n",
    "        if itv_type not in ['direct', 'general']:\n",
    "            cxt_query = [\n",
    "                'your most frequent thoughts',\n",
    "                'your emotions now', \n",
    "                'you', \n",
    "                'other people', \n",
    "                'your priorities or goals', \n",
    "                'your interests or wishes', \n",
    "                'your level of urge or motivation',\n",
    "                'your difficulties', \n",
    "            ]\n",
    "            query_thoughts = cxt_query + self.qna_cfg.sym_query\n",
    "        \n",
    "        queries = []\n",
    "        for thought in query_thoughts:\n",
    "            if thought == 'none': continue\n",
    "            if itv_type == 'general':\n",
    "                q = f\"Question: What are {thought}?\"\n",
    "                \n",
    "            if itv_type in ['direct']:\n",
    "                q = f\"Question: Do you have {thought}?\"\n",
    "                \n",
    "            if itv_type not in ['direct', 'general']:\n",
    "                if thought in self.qna_cfg.sym_query: q = f\"Question: Do you have {thought}?\"\n",
    "                if thought in cxt_query: q = f\"Question: What are {thought}?\"\n",
    "            \n",
    "            queries.append(q)\n",
    "            random.shuffle(queries) # shuffle the queries\n",
    "            \n",
    "        return queries\n",
    "        \n",
    "    def load_input_chat(\n",
    "        self,\n",
    "        sample_id,\n",
    "        step : int, \n",
    "        itv_type: str, \n",
    "        itv_thought: str,\n",
    "        query_batch: List[str],\n",
    "        out_df: pd.DataFrame,\n",
    "    ):\n",
    "        \"\"\"Load chat history for a given sample.\"\"\"\n",
    "        chat_list = []\n",
    "        \n",
    "        for query in query_batch:\n",
    "            \n",
    "            prev_chat = []\n",
    "            if step > 1:\n",
    "                _itv_type = 'phase_3' if (step == (self.qna_cfg.num_steps + 1)) else itv_type # load base chat at step (self.qna_cfg.num_steps + 1)\n",
    "                _itv_type = 'phase_4' if itv_type in ['reward', 'punish', 'forget', 'acting'] else _itv_type\n",
    "                prev_q = out_df.loc[(out_df['sample_id'] == sample_id) & (out_df['step'] == (step-1)) & (out_df['itv_type'] == _itv_type) & (out_df['itv_thought'] == itv_thought)]['query'].reset_index(drop=True)\n",
    "                prev_a = out_df.loc[(out_df['sample_id'] == sample_id) & (out_df['step'] == (step-1)) & (out_df['itv_type'] == _itv_type) & (out_df['itv_thought'] == itv_thought)]['output_text'].reset_index(drop=True)\n",
    "\n",
    "                for (q, a) in list(zip(prev_q, prev_a)):\n",
    "                    if q == query: continue # skip the current query from the previous chat\n",
    "                    prev_chat.extend([{'role': 'user', 'content': q},\n",
    "                                      {'role': 'assistant', 'content': a}])\n",
    "                \n",
    "            prev_chat.extend([{'role': 'user', 'content': query + ' ' + self.instructions[itv_type]}])\n",
    "            chat_list.append(prev_chat)\n",
    "            \n",
    "        return chat_list\n",
    "\n",
    "class QnA_Manager:\n",
    "    \"\"\"Handles thought intervention and activation modifications.\"\"\"\n",
    "    \n",
    "    def __init__(self, qna_cfg, sae, label_dict):\n",
    "        self.qna_cfg = qna_cfg\n",
    "        self.sae = sae\n",
    "        self.label_dict = label_dict\n",
    "\n",
    "    def generate_text_w_itv(self, itv_type, chat, model, tokenizer, itv_thought, itv_str_list):\n",
    "        \"\"\"Generate text based on chat history.\"\"\"\n",
    "        counter = []\n",
    "        def modify_activations(activations, hook):\n",
    "            if activations.shape[1] > 1:\n",
    "                return activations \n",
    "            else:\n",
    "                counter.append(1)\n",
    "                base_norm = activations.norm(dim=2)\n",
    "                itv_norm = itv_W_batch.norm(dim=2)            \n",
    "                alpha_itv = (base_norm / itv_norm)\n",
    "                alpha_itv = torch.mul(alpha_itv, itv_str_tensor)\n",
    "                activations = activations + (itv_W_batch * alpha_itv[:, :, None]) \n",
    "                return activations \n",
    "        \n",
    "        if (itv_type in ['phase_4', 'forget', 'acting', 'punish', 'reward']) or (itv_thought == 'none'):\n",
    "            output_text = self.generate_text(chat, model, tokenizer)\n",
    "        else:\n",
    "            itv_str_tensor, itv_W_batch = self.prepare_itv_tensors(self.label_dict[itv_thought], itv_str_list)\n",
    "            model.add_hook(self.qna_cfg.hook_name, modify_activations)\n",
    "            output_text = self.generate_text(chat, model, tokenizer)\n",
    "            model.remove_all_hook_fns()\n",
    "            \n",
    "        return output_text\n",
    "    \n",
    "    def generate_text(self, chat, model, tokenizer):\n",
    "        with torch.no_grad():\n",
    "            tokens = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\", padding=True).to(self.qna_cfg.device)\n",
    "            output_text = model.generate(tokens, max_new_tokens=self.qna_cfg.max_new_tokens, temperature=self.qna_cfg.tmp, do_sample=self.qna_cfg.do_sample, verbose=False)[:, tokens.shape[1]:]\n",
    "            output_text = tokenizer.batch_decode(output_text, skip_special_tokens=True)\n",
    "        \n",
    "        for i, text in enumerate(output_text):\n",
    "            output_text[i] = re.sub(r'\\n', ' ', text).strip() # remove newlines\n",
    "            \n",
    "        return output_text\n",
    "    \n",
    "    def prepare_itv_tensors(self, \n",
    "        itv_idx: Optional[int],\n",
    "        str_list: List[float]\n",
    "    ):\n",
    "        \n",
    "        \"\"\"Prepare tensors needed for intervention.\"\"\"\n",
    "        itv_str_tensor = torch.tensor(str_list).unsqueeze(1).to(self.qna_cfg.device)\n",
    "        itv_W = self.sae.decoder.weight.T[itv_idx].to(self.qna_cfg.device)\n",
    "        itv_W_batch = itv_W.repeat(len(str_list), 1).unsqueeze(1)\n",
    "        return itv_str_tensor, itv_W_batch\n",
    "\n",
    "class Measure_Manager:\n",
    "    def __init__(self, qna_cfg, model, tokenizer, sae, label_dict, out_df):\n",
    "        self.qna_cfg = qna_cfg\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sae = sae\n",
    "        self.label_dict = label_dict\n",
    "    \n",
    "    def sae_measure(self, output_text, hook_name):\n",
    "        with torch.no_grad():\n",
    "            output_act = self.model.run_with_cache(output_text)[1][hook_name].mean(1).to(self.qna_cfg.device)\n",
    "            X_hat, Z, Y_hat = self.sae(output_act)\n",
    "        sae_preds = Z[:, :(len(label_dict))].float().detach().cpu().numpy().tolist()\n",
    "        return sae_preds\n",
    "\n",
    "        \n",
    "    def measure_thought(self, output_text, hook_name):\n",
    "        sae_preds = self.sae_measure(output_text, hook_name)\n",
    "        return sae_preds\n",
    "\n",
    "class ThoughtQuerySystem:\n",
    "    \"\"\"Main system for handling thought queries and intervention.\"\"\"\n",
    "    \n",
    "    def __init__(self, qna_cfg, model: Any, tokenizer: Any, sae: Any, label_dict: Dict[str, Any], out_df: pd.DataFrame):\n",
    "        \n",
    "        self.qna_cfg = qna_cfg\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.out_df = out_df\n",
    "\n",
    "        self.qna = QnA_Manager(self.qna_cfg, sae, label_dict)\n",
    "        self.cm = Chat_Manager(self.qna_cfg, label_dict)\n",
    "        self.mm = Measure_Manager(self.qna_cfg, model, tokenizer, sae, label_dict, out_df)\n",
    "\n",
    "    def update_results(\n",
    "        self, \n",
    "        sample_id,\n",
    "        step : int, \n",
    "        itv_type: str, \n",
    "        query_list: Dict[int, str],\n",
    "        itv_thought: str, \n",
    "        itv_str_list: List[float], \n",
    "        output_text: Dict[int, str], \n",
    "        sae_preds: Tensor,\n",
    "    ):\n",
    "    \n",
    "        \"\"\"Update chat histories with generated output text\"\"\"\n",
    "        out_df = pd.DataFrame({\n",
    "            'sample_id': sample_id,\n",
    "            'step': step,\n",
    "            'itv_type': itv_type,\n",
    "            'itv_thought': itv_thought,\n",
    "            'sae_preds': sae_preds,\n",
    "            'query': query_list,\n",
    "            'output_text': output_text,\n",
    "        })\n",
    "        path = f'{self.qna_cfg.output_dir}'\n",
    "        out_df.to_csv(\n",
    "            path,\n",
    "            mode = 'a' if os.path.exists(path) else 'w',\n",
    "            header=not os.path.exists(path),\n",
    "            index=False\n",
    "        )\n",
    "        self.out_df = pd.concat([self.out_df, out_df], ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "    def process_thought_queries(self, \n",
    "        label_dict: Dict[str, Any], \n",
    "        itv_type: str,\n",
    "    ):\n",
    "        \"\"\"Process thought queries and save results.\"\"\"\n",
    "        \n",
    "        itv_t_list = self.qna_cfg.sym_query \n",
    "        if self.qna_cfg.itv_t: \n",
    "            itv_t_list = self.qna_cfg.itv_t\n",
    "            \n",
    "        for itv_thought in itv_t_list: # for each thought\n",
    "            \n",
    "            print(f\"Intervene thought: {itv_thought}\")\n",
    "            \n",
    "            if self.qna_cfg.sample_id is None:\n",
    "                max_s_id = int(self.out_df.loc[(self.out_df['itv_type'] == itv_type) & (self.out_df['itv_thought'] == itv_thought)]['sample_id'].max()) if len(self.out_df.loc[(self.out_df['itv_type'] == itv_type) & (self.out_df['itv_thought'] == itv_thought)]) > 0 else 0\n",
    "                sample_id = max_s_id + 1\n",
    "            else:\n",
    "                sample_id = self.qna_cfg.sample_id\n",
    "\n",
    "            iterator = trange(self.qna_cfg.num_steps, desc='Step: ', leave=False)\n",
    "            for step in iterator:\n",
    "                if self.qna_cfg.step is None: \n",
    "                    step += (self.qna_cfg.num_steps + 1) if itv_type not in ['direct', 'general', 'phase_3'] else 1 # start from step (self.qna_cfg.num_steps + 1) for non-base interventions\n",
    "                else:\n",
    "                    step += self.qna_cfg.step\n",
    "                iterator.set_description(f\"Step: {step}\")\n",
    "                \n",
    "                query_batch = self.cm.generate_queries(itv_type)\n",
    "                chat_list = self.cm.load_input_chat(sample_id, step, itv_type, itv_thought, query_batch, self.out_df)\n",
    "                itv_str_list = self.qna_cfg.itv_strengths * (len(query_batch) // len(self.qna_cfg.itv_strengths))\n",
    "\n",
    "                tf = ['What are' in query for query in query_batch]\n",
    "                itv_str_list = [itv_str_list[i] if tf[i] else 0 for i in range(len(tf))]\n",
    "\n",
    "                output_text = []\n",
    "                for i in range(0, len(query_batch), self.qna_cfg.batch_size):\n",
    "                    output_text.extend(self.qna.generate_text_w_itv(itv_type, chat_list[i:i+self.qna_cfg.batch_size], model, tokenizer, itv_thought, itv_str_list[i:i+self.qna_cfg.batch_size]))\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                sae_preds = self.mm.measure_thought(output_text, self.qna_cfg.hook_name)\n",
    "                self.update_results(sample_id, step, itv_type, query_batch, itv_thought, itv_str_list, output_text, sae_preds)\n",
    "                \n",
    "                \n",
    "        return self.out_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "RUN Q&A\n",
    "\"\"\"\n",
    "\n",
    "# run Q&A for the Fig.3(a) - left of the vertical line\n",
    "qna_cfg = QNA_Config()\n",
    "qna_cfg.itv_type = 'phase_3'\n",
    "sae.eval().to(qna_cfg.device)\n",
    "out_df = pd.read_csv(qna_cfg.output_dir)\n",
    "system = ThoughtQuerySystem(qna_cfg, model, tokenizer, sae, label_dict, out_df)\n",
    "out_df = system.process_thought_queries(label_dict, itv_type=qna_cfg.itv_type)\n",
    "\n",
    "# run Q&A for the Fig.3(a) - left of the vertical line\n",
    "qna_cfg = QNA_Config()\n",
    "qna_cfg.itv_type = 'phase_4'\n",
    "sae.eval().to(qna_cfg.device)\n",
    "out_df = pd.read_csv(qna_cfg.output_dir)\n",
    "system = ThoughtQuerySystem(qna_cfg, model, tokenizer, sae, label_dict, out_df)\n",
    "out_df = system.process_thought_queries(label_dict, itv_type=qna_cfg.itv_type)\n",
    "\n",
    "# run Q&A for the Fig.3(e) - activation by defense prompts\n",
    "qna_cfg = QNA_Config()\n",
    "itv_types = ['reward', 'punish', 'forget', 'acting']\n",
    "for itv_type in itv_types:\n",
    "    qna_cfg.itv_type = itv_type\n",
    "    qna_cfg.itv_t = ['none'] + list(label_dict.keys())\n",
    "    qna_cfg.step = 101\n",
    "    qna_cfg.num_steps = 1\n",
    "    sae.eval().to(qna_cfg.device)\n",
    "    out_df = pd.read_csv(qna_cfg.output_dir)\n",
    "    system = ThoughtQuerySystem(qna_cfg, model, tokenizer, sae, label_dict, out_df)\n",
    "    out_df = system.process_thought_queries(label_dict, itv_type=qna_cfg.itv_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
